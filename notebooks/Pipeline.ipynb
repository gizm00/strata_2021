{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expanded-graphics",
   "metadata": {},
   "source": [
    "![Many pancakes vs single pancake](images/pancakes.png)\n",
    "\n",
    "#### How might our camping application scale?\n",
    "* More states\n",
    "* Additional data sources\n",
    "* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "furnished-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from camping.mocks.request import RequestsMock\n",
    "from camping.util.scraper import Scraper\n",
    "from camping.util.distance import distance_merge\n",
    "\n",
    "def max_col_width(w=100):\n",
    "    pd.set_option('display.max_colwidth', w)\n",
    "\n",
    "ridb_facilities_url = \"https://ridb.recreation.gov/api/v1/facilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-newman",
   "metadata": {},
   "source": [
    "### Scaling our prototype code\n",
    "\n",
    "What are some aspects that might not scale?   \n",
    "*   \n",
    "  \n",
    "Opportunities to parallelize?  \n",
    "*   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-classics",
   "metadata": {},
   "source": [
    "#### Exploration code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"activity_id\":9, \"state\":\"OR\"}\n",
    "headers = {\"accept\": \"application/json\", \"apikey\": \"key\"}\n",
    "\n",
    "# Get RIDB facilities data\n",
    "response = RequestsMock.get(ridb_facilities_url, params, headers=headers)\n",
    "camping_json  = json.loads(response.text)\n",
    "df_ridb_camping = pd.DataFrame(camping_json['RECDATA'])\n",
    "\n",
    "# Get RIDB campground data for each facility\n",
    "campground_info = pd.DataFrame()\n",
    "for facility in camping_json['RECDATA']:\n",
    "    if facility.get('FacilityID') is not None:\n",
    "        campground_url = f\"{ridb_facilities_url}/{facility['FacilityID']}/campsites\"\n",
    "        resp = RequestsMock.get(campground_url, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            continue\n",
    "        \n",
    "        campsites = json.loads(resp.text)\n",
    "        if len(campsites['RECDATA']) > 0:\n",
    "            df_campsites = pd.DataFrame(campsites['RECDATA'])\n",
    "            campground_info = campground_info.append(df_campsites[['FacilityID', 'CampsiteID', 'CampsiteName', 'ATTRIBUTES']].merge(df_ridb_camping, on='FacilityID', how='left'))\n",
    "\n",
    "# Get NF website data\n",
    "nf_data = []\n",
    "with open('../data/NF_sites/OR_sitelist.csv') as f:\n",
    "    reader = DictReader(f)\n",
    "    for row in reader:\n",
    "        sc = Scraper(row['site_url'], row['site_name'])\n",
    "        nf_data.append(sc.scrape())\n",
    "nf_df = pd.DataFrame(nf_data) \n",
    "\n",
    "# Merge RIDB and NF data\n",
    "merged = distance_merge(nf_df, campground_info, 2000, 'ridb', 'nf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-panic",
   "metadata": {},
   "source": [
    "\n",
    "### Pipelines\n",
    "Data pipelines split data processing into discrete steps. To get a sense of how to break our code into steps, lets take a look at the data processing flow of our prototype:\n",
    "\n",
    "Single pancake. takes so long!\n",
    "\n",
    "![Prototype Flow](images/prototype_flow.png)\n",
    "\n",
    "\n",
    "Data processing steps in our prototype:\n",
    "* Extracting the data from source\n",
    "* Transforming campsite data\n",
    "* Merging data into a single table\n",
    "* Storing the table in a database (not pictured)\n",
    "\n",
    "Consider...\n",
    "* What happens if there is an error in convert attributes for one campsite?\n",
    "* How long will it take to run if a facility has a large number of campsites?\n",
    "\n",
    "\n",
    "Pipelines enable scaling up of data processing\n",
    "* Reduced latency through parallel processing\n",
    "* Ability to take advantage of \"near limitless\" cloud compute\n",
    "* Generalized pipeline steps can be reused \n",
    "* Resiliency - data can be saved to disk or cloud storage after a stage to allow retry \n",
    "\n",
    "\n",
    "What can we parallelize ?\n",
    "* Process campsite data in batches\n",
    "* Data source extraction - NF can run independently of RIDB\n",
    "\n",
    "Configuration to scale up without code modifications\n",
    "* State\n",
    "* NF urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-secret",
   "metadata": {},
   "source": [
    "#### Lets start by breaking our data processing code into discrete steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "plastic-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with getting facilities\n",
    "def get_facilities(state):\n",
    "    params = {'state':state}\n",
    "    response = RequestsMock.get(ridb_facilities_url, params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Getting facilities for {state}\")\n",
    "        result = json.loads(response.text)\n",
    "        return result['RECDATA']\n",
    "    print(f\"Unable to get result for state {state}, got {response.reason}\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "advised-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campsites(facility_id):\n",
    "    campsite_details_url = f\"ridb_facilities_url/{facility_id}/campsites\"\n",
    "    response = RequestsMock.get(campsite_details_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        campsites = json.loads(response.text)\n",
    "        if len(campsites['RECDATA']) > 0:\n",
    "            return campsites['RECDATA']\n",
    "        else:\n",
    "            return {}\n",
    "    print(f\"Unable to get result for facility_id {facility_id}, got {response.code} {response.reason}\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "peripheral-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_campsites(campsite_data):\n",
    "    # Another benefit of breaking data processing code into steps is encapsulation -\n",
    "    # if we need to do additional processing on campsite data at a later time\n",
    "    # we only need to modify this method. Also helpful for unit testing.\n",
    "    campsite_data['AttributeDict'] = {item['AttributeName']: item['AttributeValue'] for item in campsite_data['ATTRIBUTES']}\n",
    "    return {key: campsite_data.get(key) for key in ['FacilityID', 'CampsiteID', 'CampsiteName', 'AttributeDict']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-orlando",
   "metadata": {},
   "source": [
    "#### Batch processing pipeline\n",
    "\n",
    "Lets say we got facility IDs 9000 - 9999, here is an example of how we could run parallel batches.  \n",
    "\n",
    "Big data engines like Spark can do this for you if you take advantage of distributed data structures.  \n",
    "Keep in mind the impact of parallel API calls!\n",
    "\n",
    "Many pancakes...\n",
    "\n",
    "![Batch processing](images/campsite_batch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "refined-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_campsite_batch(facility_ids):\n",
    "    campsite_data = []\n",
    "    for id in facility_ids:\n",
    "        # In a data pipeline get_campsites and process_campsites would be different stages.\n",
    "        # To illustrate running the batch we call both in this method\n",
    "        raw_data = get_campsites(id)\n",
    "        if raw_data == {}:\n",
    "            continue\n",
    "        for site in raw_data:\n",
    "            campsite_data.append(process_campsites(site))\n",
    "    return campsite_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "broad-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting facilities for OR\n"
     ]
    }
   ],
   "source": [
    "states = ['OR']\n",
    "facilities_json = get_facilities('OR')\n",
    "facility_ids = [f['FacilityID'] for f in facilities_json]\n",
    "num_facilities = len(facility_ids)\n",
    "step = int(num_facilities/10)\n",
    "for index in range(0, num_facilities, step):\n",
    "    process_campsite_batch(facility_ids[index:index+step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-discussion",
   "metadata": {},
   "source": [
    "#### How about forest service data?\n",
    "Is there anything differently you would do here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "terminal-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_data(source_file):\n",
    "    nf_data = []\n",
    "    with open(source_file) as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            # Can parallelize this as well\n",
    "            sc = Scraper(row['site_url'], row['site_name'])\n",
    "            nf_data.append(sc.scrape())\n",
    "    return nf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-richards",
   "metadata": {},
   "source": [
    "![RIDB pipeline](images/full_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What stages might we want to be able to retry?\n",
    "# Where might we want to save our progress?\n",
    "# How about scheduling? Might not be time for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "noble-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with what we want as paramaterized inputs and build from there\n",
    "NF_sites = [\n",
    "    (\"East Lemolo Campground\", \"https://www.fs.usda.gov/recarea/umpqua/recarea/?recid=63492\"),\n",
    "    (\"Magone Lake Campground\", \"https://www.fs.usda.gov/recarea/malheur/recarea/?recid=39964\")\n",
    "]\n",
    "states = ['OR', 'WA', 'CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "formal-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse - propane canister\n",
    "def get_ridb_data(url, params, headers):\n",
    "    response = RequestsMock.get(url, params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        result = json.loads(response.text)\n",
    "        return result['RECDATA']\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-attitude",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
