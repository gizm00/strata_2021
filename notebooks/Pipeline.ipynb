{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expanded-graphics",
   "metadata": {},
   "source": [
    "![Many pancakes vs single pancake](images/pancakes.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "furnished-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from camping.mocks.request import RequestsMock\n",
    "from camping.util.scraper import Scraper\n",
    "from camping.util.distance import distance_merge\n",
    "\n",
    "def max_col_width(w=100):\n",
    "    pd.set_option('display.max_colwidth', w)\n",
    "\n",
    "ridb_facilities_url = \"https://ridb.recreation.gov/api/v1/facilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-newman",
   "metadata": {},
   "source": [
    "### Scaling our prototype code\n",
    "\n",
    "What are some aspects that might not scale?   \n",
    "*   \n",
    "  \n",
    "Opportunities to parallelize?  \n",
    "*   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-classics",
   "metadata": {},
   "source": [
    "#### Exploration code for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approximate-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting facility data\n",
      "Getting campsite data\n",
      "Getting NF data\n",
      "Merging data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gizmo/.pyenv/versions/3.8.5/envs/strata_venv/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "/Users/gizmo/.pyenv/versions/3.8.5/envs/strata_venv/lib/python3.8/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "def transform_campsites(campsite_json):\n",
    "    # First, translate the ATTRIBUTES list into a list of dict of {AttributeName: AttributeValue}\n",
    "    for i in range(len(campsite_json)):\n",
    "        campsite_json[i]['AttributeDict'] = [{item['AttributeName']: item['AttributeValue']} for item in campsite_json[i]['ATTRIBUTES']]\n",
    "\n",
    "    # Next, convert the AttriuteDict to columns\n",
    "    df = pd.DataFrame(campsite_json)[['ATTRIBUTES', 'CampsiteID', 'CampsiteName', 'FacilityID']]\n",
    "    df['AttributeDict'] = df['ATTRIBUTES'].apply(lambda x: {item['AttributeName']: item['AttributeValue'] for item in x})\n",
    "    norm = pd.json_normalize(df['AttributeDict'])\n",
    "    df = df[['CampsiteID','CampsiteName','FacilityID']].join(norm)\n",
    "    return df.replace({np.nan: \"\"})\n",
    "\n",
    "\n",
    "ridb_facilities_url = \"https://ridb.recreation.gov/api/v1/facilities\"\n",
    "params = {\"activity_id\":9, \"state\":\"OR\"}\n",
    "headers = {\"accept\": \"application/json\", \"apikey\": \"key\"}\n",
    "campground_info = pd.DataFrame()\n",
    "\n",
    "# Get RIDB Facilities with camping\n",
    "print(\"Getting facility data\")\n",
    "response = RequestsMock.get(ridb_facilities_url, params, headers=headers)\n",
    "camping_json  = json.loads(response.text)\n",
    "df_ridb_camping = pd.DataFrame(camping_json['RECDATA'])\n",
    "\n",
    "# Get campsite data for each facility\n",
    "print(\"Getting campsite data\")\n",
    "for facility in camping_json['RECDATA']:\n",
    "    campground_url = f\"{ridb_facilities_url}/{facility['FacilityID']}/campsites\"\n",
    "    resp = RequestsMock.get(campground_url, headers=headers)\n",
    "    campsites = json.loads(resp.text)\n",
    "    if len(campsites['RECDATA']) > 0:\n",
    "        df_campsites = transform_campsites(campsites['RECDATA'])\n",
    "        campground_info = campground_info.append(df_campsites.merge(df_ridb_camping, on='FacilityID', how='left'))\n",
    "\n",
    "# Get NF webscraper data\n",
    "print(\"Getting NF data\")\n",
    "nf_data = []\n",
    "with open('../data/NF_sites/OR_sitelist.csv') as f:\n",
    "    reader = DictReader(f)\n",
    "    for row in reader:\n",
    "        sc = Scraper(row['site_url'], row['site_name'])\n",
    "        nf_data.append(sc.scrape())\n",
    "nf_df = pd.DataFrame(nf_data)\n",
    "\n",
    "print(\"Merging data\")\n",
    "merged_sites = distance_merge(nf_df, campground_info, 2000, 'ridb', 'nf')\n",
    "merged_sites.drop(columns=['FacilityLatitude_nf', 'FacilityLongitude_nf', 'index_nf', 'FacilityLongitude_ridb', 'FacilityLatitude_ridb', 'FacilityName', 'geometry'], inplace=True)\n",
    "combined = campground_info.merge(merged_sites, how='left', on=['FacilityID','CampsiteID'])\n",
    "combined = combined.replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-panic",
   "metadata": {},
   "source": [
    "\n",
    "### Pipelines\n",
    "Data pipelines split data processing into discrete steps.  \n",
    "Pipelines enable scaling up of data processing:  \n",
    "* Reduced latency through parallel processing\n",
    "* Ability to take advantage of \"near limitless\" cloud compute\n",
    "* Generalized pipeline steps can be reused \n",
    "* Resiliency - data can be saved to disk or cloud storage after a stage to allow retry \n",
    "\n",
    "\n",
    "To get a sense of how to break our code into steps, lets take a look at the data processing flow of our prototype:\n",
    "\n",
    "![Prototype Flow](images/prototype_flow.png)\n",
    "\n",
    "\n",
    "Data processing steps in our prototype:\n",
    "* Extracting the data from source\n",
    "* Transforming campsite data\n",
    "* Merging data into a single table\n",
    "* Storing the table in a database (not pictured)\n",
    "\n",
    "Consider...\n",
    "* What happens if there is an error in convert attributes for one campsite?\n",
    "* How long will it take to run if a facility has a large number of campsites? $$\n",
    "\n",
    "\n",
    "What can we parallelize ?\n",
    "* Process campsite data in batches\n",
    "* Data source extraction - NF web scraper can run independently of RIDB API calls\n",
    "* Run multiple states in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-pension",
   "metadata": {},
   "source": [
    "### Scaling up - data structures and formats\n",
    "\n",
    "**Ask yourself** \n",
    "* Are the data structures I'm using optimized for large scale?\n",
    "* The data pipeline is a means to an end - what is acting on the data being produced? For example, APIs surface json to front end frameworks, do you really need to put all of this information in a table?\n",
    "\n",
    "\n",
    "##### Getting facilities data\n",
    "We're converting the facilities to a dataframe so we can explore it, but its not necessary for the pipeline\n",
    "```python\n",
    "# Get RIDB Facilities with camping\n",
    "response = RequestsMock.get(ridb_facilities_url, params, headers=headers)\n",
    "camping_json  = json.loads(response.text)\n",
    "> df_ridb_camping = pd.DataFrame(camping_json['RECDATA'])\n",
    "```\n",
    "  \n",
    "---------  \n",
    "  \n",
    "##### Campsite transformation code\n",
    "We're doing some expensive operations to convert the ATTRIBUTES list of dict into columns. This was really useful for data exploration, but unnecessarily expensive.  \n",
    "  \n",
    "Also, surfacing the attributes as table columns makes our system brittle. Consider if new attributes are added - if we had to convert these to columns it would require a database migraiton for our pipeline output.\n",
    "\n",
    "```python\n",
    "def transform_campsites(campsite_json):\n",
    ">    df = pd.DataFrame(campsite_json)[['ATTRIBUTES', 'CampsiteID', 'CampsiteName', 'FacilityID']]\n",
    "    \n",
    "    # First, translate the ATTRIBUTES list into a list of dict of {AttributeName: AttributeValue}\n",
    ">    df['AttributeDict'] = df['ATTRIBUTES'].apply(lambda x: {item['AttributeName']: item['AttributeValue'] for item in x})\n",
    "    \n",
    "    # Create a dataframe with the keys of AttributeDict as column names, values as rows\n",
    ">    norm = pd.json_normalize(df['AttributeDict'])\n",
    "    \n",
    "    # Join the normalized data back to the rest of the campground data, dropping the ATTRIBUTES column\n",
    ">    df = df[['CampsiteID','CampsiteName','FacilityID']].join(norm)\n",
    "    return df.replace({np.nan: \"\"})\n",
    "```\n",
    "\n",
    "**Keep in mind**  \n",
    "Keeping data in its original format will help your pipeline be robust to data source changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satisfied-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate expensive apply and join, improve resiliency to data source changes\n",
    "def transform_campsites(campsite_json):\n",
    "    for i in range(len(campsite_json)):\n",
    "        campsite_json[i]['AttributeDict'] = [{item['AttributeName']: item['AttributeValue']} for item in campsite_json[i]['ATTRIBUTES']]\n",
    "    return campsite_json\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-hawaii",
   "metadata": {},
   "source": [
    "### Scaling up through parallel processing - Infrastructure\n",
    "\n",
    "**Ask yourself:** What parts of my data processing flow are independent?\n",
    "\n",
    "* We can process the NF web data in parallel with RIDB\n",
    "* We can also process each state independently\n",
    "\n",
    "The figure below shows independent pipelines running for Oregon and Washington\n",
    "\n",
    "**Keep in mind**\n",
    "* How will you track API use in a distributed system like this?\n",
    "\n",
    "![RIDB pipeline](images/OR_WA_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-remark",
   "metadata": {},
   "source": [
    "### Scaling up through parallel processing - Distributed data strucutres\n",
    "\n",
    "**Ask yourself**  \n",
    "What processes operate over a large number of the same data? For loops are a good place to look.\n",
    "\n",
    "[Spark](https://spark.apache.org/) and [Dask](https://docs.dask.org/en/latest/) are examples of big data processing engines that have distributed data frame structures, enabling you to spread computation across many compute nodes to speed up runtime.  \n",
    "\n",
    "In our case, campsite processing is a good candidate for this, as well as the NF web scraping. \n",
    "\n",
    "**Keep in mind**  \n",
    "Similar to parallel infrastrucutre, keep in mind the impact of parallelizing calls to external services \n",
    "\n",
    "![Batch processing](images/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-stomach",
   "metadata": {},
   "source": [
    "### Scaling up - building in resiliency\n",
    "\n",
    "**Ask yourself** What are potential points of failure in my pipeline, and how could caching data help reduce time and expenses?\n",
    "\n",
    "Consider what happens in our prototype if a merge fails - we need to go back to the beginning and regenrate all the data. This can be costly when working with large scale data, building in caching and retry logic can help.\n",
    "\n",
    "When thinking about separating steps into independent pipeline tasks:\n",
    "* Similar to thinking about object oriented principles when coding, think about single use and encapsulation when breaking data processing code into pipeline stages\n",
    "* Consider persisting data that takes a long time to generate  \n",
    "\n",
    "![Batch processing](images/retry_on_fail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-devon",
   "metadata": {},
   "source": [
    "### Scaling up with configurable components\n",
    "\n",
    "**Ask yourself** \n",
    "* If I were thinking about my data processing code as a function, what would make sense to parameterize?\n",
    "* How might this system scale in breadth? For example, perhaps we want to find a place to camp that is at a facility that also has boating (`activity_id: 6`). It might be interesting to know what events are going on at the facility as well, availabile at the `/facilities/{facilityId}/events` endpoint. \n",
    "\n",
    "Workflow management tools like Apache [Airflow](https://airflow.apache.org/) can help you setup these kinds of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "based-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ridb_data(url, headers, params={}):\n",
    "    response = RequestsMock.get(url, params, headers)\n",
    "    if response.status_code == 200:\n",
    "        result = json.loads(response.text)\n",
    "        return result['RECDATA']\n",
    "    return {}\n",
    "\n",
    "RIDB_FACILITIES_URL = \"https://ridb.recreation.gov/api/v1/facilities\"\n",
    "HEADERS = {\"accept\": \"application/json\", \"apikey\": \"key\"}\n",
    "\n",
    "pipeline_config = [\n",
    "    {'label': 'OR', 'nf_sites': '../data/NF_sites/OR_sitelist.csv', 'params':{'state': 'OR', 'activity_id': '9,6'}},\n",
    "    {'label': 'WA', 'nf_sites': '../data/NF_sites/WA_sitelist.csv', 'params':{'state': 'WA', 'activity_id': '9'}}]\n",
    "\n",
    "\n",
    "# For demonstrating config\n",
    "def run_pipeline(config):\n",
    "    results = {}\n",
    "    \n",
    "    # Each entry in the config could spawn a new pipeline job\n",
    "    for item in config:\n",
    "        # Each of these method calls could be a pipeline step\n",
    "        facilities = get_ridb_data(RIDB_FACILITIES_URL, HEADERS, item['params'])\n",
    "        campsite_data = get_campsite_data(facilities)\n",
    "        nf_data = get_nf_data(item['nf_sites'])\n",
    "        \n",
    "        # Additional steps to process other data sources \n",
    "        \n",
    "        results[item['label']] = merge_data(facilities, campsite_data, nf_data)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_campsite_data(facilities):\n",
    "    data = []\n",
    "    for facility in facilities:\n",
    "        url = f\"RIDB_FACILITIES_URL/{facility['FacilityID']}/campsites\"\n",
    "        campsite_data = get_ridb_data(url, HEADERS)\n",
    "        if campsite_data != []:\n",
    "            data.append(transform_campsites(campsite_data))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Keeping transformation code seperate makes it easier to test and modify without impacting\n",
    "# extraction and loading code\n",
    "def transform_campsites(campsite_json):\n",
    "    for i in range(len(campsite_json)):\n",
    "        campsite_json[i]['AttributeDict'] = [{item['AttributeName']: item['AttributeValue']} for item in campsite_json[i]['ATTRIBUTES']]\n",
    "    return campsite_json\n",
    "      \n",
    "    \n",
    "def get_nf_data(file_name):\n",
    "    nf_data = []\n",
    "    with open(file_name) as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            sc = Scraper(row['site_url'], row['site_name'])\n",
    "            nf_data.append(sc.scrape())\n",
    "    return pd.DataFrame(nf_data) \n",
    "\n",
    "def merge_data(facilities, campsite_data, nf_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-sight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
